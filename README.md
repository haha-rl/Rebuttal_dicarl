# Rebuttal supplements of DICARL

<div align=center><img src = "https://github.com/haha-rl/Rebuttal_dicarl/blob/main/Heatmap-return.jpg"></div>
<div align=center>Figure 1. Average accumulative reward across seven seeds on each test set. The x- and y-axes represent the mass changes of different parts of the robot, respectively.</div>


<div align=center><img src = "https://github.com/haha-rl/Rebuttal_dicarl/blob/main/Heatmap-sac.jpg"  width=600 alt="figure"></div>
<div align=center>Figure 2. The average failure rate of Vanilla-SAC, RARL-SAC and DICARL-SAC algorithms. Each experiment is run for 1e6 timesteps and is repeated 5 times.</div>

<div align=center><img src = "https://github.com/haha-rl/Rebuttal_dicarl/blob/main/Training-curve.jpg"></div>
<div align=center>Figure 3. The training curves of Vanilla, RARL, NR-MDP, Oracle, DICARL and RLAC algorithms.</div>

<div align=center><img src = "https://github.com/haha-rl/Rebuttal_dicarl/blob/main/DICARLvsRLAC.gif"></div>
<div align=center>Figure 4. Visualization of DICARL and RLAC agents in four MuJoCo environments.</div>

